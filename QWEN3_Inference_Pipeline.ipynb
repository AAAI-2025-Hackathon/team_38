{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb40e2675ebf4d40a084910cef7a073c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a97761db5ebb420cab642360887ccd25",
              "IPY_MODEL_003fb873bbbc483b8ff69ffbe5947122",
              "IPY_MODEL_ded756bb2911486080486a2810d56c94"
            ],
            "layout": "IPY_MODEL_89722e1d457449e39f1b6017bc1f3695"
          }
        },
        "a97761db5ebb420cab642360887ccd25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf468ef8de9c4fd087e562b76caa1494",
            "placeholder": "​",
            "style": "IPY_MODEL_16ddcf142ba747be9869d7938ee94fb6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "003fb873bbbc483b8ff69ffbe5947122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_147ce7804bdf4c30839bd48a4026f3eb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_632bf43dce07456ea05ea3f99252ece6",
            "value": 2
          }
        },
        "ded756bb2911486080486a2810d56c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e8a7b0ae7754679b4bdb46222cf8d54",
            "placeholder": "​",
            "style": "IPY_MODEL_15d6368123e942bf8b8848d4e36c6e3f",
            "value": " 2/2 [00:22&lt;00:00, 10.34s/it]"
          }
        },
        "89722e1d457449e39f1b6017bc1f3695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf468ef8de9c4fd087e562b76caa1494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16ddcf142ba747be9869d7938ee94fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "147ce7804bdf4c30839bd48a4026f3eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "632bf43dce07456ea05ea3f99252ece6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e8a7b0ae7754679b4bdb46222cf8d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d6368123e942bf8b8848d4e36c6e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers torch transformers_stream_generator optimum auto-gptq tiktoken"
      ],
      "metadata": {
        "id": "5yOMK7BTi6qp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import io\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "import optimum\n",
        "# from google.colab import files"
      ],
      "metadata": {
        "id": "Q16FQE82i8GT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV files into DataFrames\n",
        "forecast_arima = pd.read_csv('/content/drive/MyDrive/AAAIHack/bond_yields_predicted_GAN.csv')\n",
        "forecast_garima = pd.read_csv('/content/drive/MyDrive/AAAIHack/bond_yields_predicted_actual.csv')\n",
        "past_year_data = pd.read_csv('/content/drive/MyDrive/AAAIHack/bond_yields_previous_FY.csv')"
      ],
      "metadata": {
        "id": "DiECIw88i_GC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the DataFrames to CSV formatted strings (without the index)\n",
        "arima_str = forecast_arima.to_csv(index=False)\n",
        "garima_str = forecast_garima.to_csv(index=False)\n",
        "past_year_str = past_year_data.to_csv(index=False)"
      ],
      "metadata": {
        "id": "luBJWobHjAVA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = f\"\"\"\n",
        "You are a senior financial advisor specializing in fixed income and bond markets, with over 20 years of experience. Your expertise covers analyzing market trends, forecasting yield movements, and evaluating risk factors.\n",
        "\n",
        "Using the data provided below:\n",
        "1. Forecasted bond yields for the next 3 months from two ARIMA models based off different datas\n",
        "2. Historical bond yield data for the past 12 months\n",
        "\n",
        "Please analyze these details thoroughly and offer clear investment recommendations for each of the four bonds (AAA, BAA, US10Y, and Junk). For each bond, consider:\n",
        "- Analysis of the forecasted trends\n",
        "- Comparison with historical performance\n",
        "- Consideration of yield volatility and risk-return trade-offs\n",
        "\n",
        "Provide your output in the following structured format:\n",
        "-----------------------------------------------------------\n",
        "Bond: [Bond Name]\n",
        "Recommendation: [BUY/HOLD/SELL]\n",
        "Timing: [Specify timing, e.g., \"BUY NOW\", \"BUY THIS MONTH\", \"BUY NEXT MONTH\", \"HOLD for 6 months\", \"SELL within 1 month\"]\n",
        "Risk: [Specify risk, e.g., \"LOW\", \"MEDIUM\", \"HIGH\"]\n",
        "-----------------------------------------------------------\n",
        "Ensure that your final output strictly follows this format for each bond.\n",
        "\n",
        "\n",
        "Forecasted Bond Yields for the Next 3 Months:\n",
        "--------------------------------------------\n",
        "ARIMA Forecast:\n",
        "{arima_str}\n",
        "\n",
        "GARIMA Forecast:\n",
        "{garima_str}\n",
        "\n",
        "Historical Bond Yield Data (Past 12 Months):\n",
        "--------------------------------------------\n",
        "{past_year_str}\n",
        "\n",
        "Please provide your analysis and recommendations in the structured format described.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nQuuzDXAjCBo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # -----------------------------\n",
        "# # Step 3: Set Up the LLM Pipeline\n",
        "# # -----------------------------\n",
        "# model_id = \"WiroAI/WiroAI-Finance-Qwen-7B\"\n",
        "\n",
        "# pipeline = transformers.pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model_id,\n",
        "#     model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "\n",
        "# pipeline.model.eval()\n",
        "\n",
        "# # Create the conversation messages.\n",
        "# messages = [\n",
        "#     {\"role\": \"system\", \"content\": \"You are a Senior Financial Advisor with deep expertise in fixed income investments and bond market analysis.\"},\n",
        "#     {\"role\": \"user\", \"content\": user_prompt},\n",
        "# ]\n",
        "\n",
        "# # Define terminators to mark the end of the generated text.\n",
        "# terminators = [\n",
        "#     pipeline.tokenizer.eos_token_id,\n",
        "#     pipeline.tokenizer.convert_tokens_to_ids(\"<｜end▁of▁sentence｜>\")\n",
        "# ]"
      ],
      "metadata": {
        "id": "7GJBcI6OjEMP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g3yKyBrssBD6"
      },
      "outputs": [],
      "source": [
        "# # -----------------------------\n",
        "# # Step 4: Generate Investment Advice\n",
        "# # -----------------------------\n",
        "# outputs = pipeline(\n",
        "#     messages,\n",
        "#     max_new_tokens=512,\n",
        "#     eos_token_id=terminators,\n",
        "#     do_sample=True,\n",
        "#     temperature=0.9,\n",
        "# )\n",
        "\n",
        "# # Print the generated investment advice.\n",
        "# # (Note: The output structure is based on the sample usage.)\n",
        "# print(outputs[0][\"generated_text\"][-1]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load the new, faster model from TheBloke/Qwen-7B-Chat-GPTQ\n",
        "model_name_or_path = \"TheBloke/Qwen-7B-Chat-GPTQ\"  # Optionally add \":gptq-4bit-32g-actorder_True\" for a specific branch\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    revision=\"main\"  # Change revision if needed\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537,
          "referenced_widgets": [
            "eb40e2675ebf4d40a084910cef7a073c",
            "a97761db5ebb420cab642360887ccd25",
            "003fb873bbbc483b8ff69ffbe5947122",
            "ded756bb2911486080486a2810d56c94",
            "89722e1d457449e39f1b6017bc1f3695",
            "bf468ef8de9c4fd087e562b76caa1494",
            "16ddcf142ba747be9869d7938ee94fb6",
            "147ce7804bdf4c30839bd48a4026f3eb",
            "632bf43dce07456ea05ea3f99252ece6",
            "8e8a7b0ae7754679b4bdb46222cf8d54",
            "15d6368123e942bf8b8848d4e36c6e3f"
          ]
        },
        "id": "WS_LekyT0aNZ",
        "outputId": "a7863aef-71c7-4d43-8c49-ecdb5cb944d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n",
            "WARNING:transformers_modules.TheBloke.Qwen-7B-Chat-GPTQ.eb5f2e8ef11aa658680440981de24352e06c25cb.modeling_qwen:Warning: please make sure that you are using the latest codes and checkpoints, especially if you used Qwen-7B before 09.25.2023.请使用最新模型和代码，尤其如果你在9月25日前已经开始使用Qwen-7B，千万注意不要使用错误代码和模型。\n",
            "WARNING:transformers_modules.TheBloke.Qwen-7B-Chat-GPTQ.eb5f2e8ef11aa658680440981de24352e06c25cb.modeling_qwen:Try importing flash-attention for faster inference...\n",
            "WARNING:transformers_modules.TheBloke.Qwen-7B-Chat-GPTQ.eb5f2e8ef11aa658680440981de24352e06c25cb.modeling_qwen:Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
            "WARNING:transformers_modules.TheBloke.Qwen-7B-Chat-GPTQ.eb5f2e8ef11aa658680440981de24352e06c25cb.modeling_qwen:Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
            "WARNING:transformers_modules.TheBloke.Qwen-7B-Chat-GPTQ.eb5f2e8ef11aa658680440981de24352e06c25cb.modeling_qwen:Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb40e2675ebf4d40a084910cef7a073c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at TheBloke/Qwen-7B-Chat-GPTQ were not used when initializing QWenLMHeadModel: ['transformer.h.31.mlp.c_proj.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.24.attn.c_proj.bias', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.24.mlp.w2.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.31.attn.c_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.30.mlp.w2.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.26.mlp.w1.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.27.mlp.w1.bias', 'transformer.h.29.mlp.w2.bias', 'transformer.h.25.attn.c_proj.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.27.attn.c_proj.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.27.mlp.w2.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.25.mlp.w2.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.26.mlp.w2.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.28.attn.c_proj.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.24.mlp.w1.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.29.attn.c_proj.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.25.mlp.w1.bias', 'transformer.h.29.mlp.w1.bias', 'transformer.h.28.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.31.mlp.w2.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.30.mlp.w1.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.9.mlp.w2.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.28.mlp.w2.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.30.attn.c_proj.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.31.mlp.w1.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.26.attn.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.18.mlp.w1.bias']\n",
            "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for TheBloke/Qwen-7B-Chat-GPTQ contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/TheBloke/Qwen-7B-Chat-GPTQ.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the prompt template using the required tokens for this model\n",
        "system_message = \"You are a Senior Financial Advisor with deep expertise in fixed income investments and bond market analysis.\"\n",
        "prompt_template = f\"\"\"<|im_start|>system\n",
        "{system_message}<|im_end|>\n",
        "<|im_start|>user\n",
        "{user_prompt}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "# Create the text-generation pipeline with updated parameters\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.95,\n",
        "    top_k=40,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "# Generate the investment advice using the new model\n",
        "output = pipe(prompt_template)[0]['generated_text']\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqS13hAgrOH5",
        "outputId": "ce21b472-d11f-444c-e8aa-4fc795169b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROuzKh68mxMJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}